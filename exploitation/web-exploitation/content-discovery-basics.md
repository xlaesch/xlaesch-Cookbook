# Content Discovery Basics

### Browser Tools

* **View Page Source**: allows visibility of comments, directory (if allowed), and frameworks (sometimes).
* **Inspector**: changing HTML, CSS and JS in real time.
* **Debugger:** Breakpoints can be used to force the browser to stop processing JS and pause execution.
* **Network:** viewed sent and received network data in the web application.



### Other Methods

#### Robots.txt:

A file in the web application used to tell search engines which websites to show an not. Can give indications of important hidden websites or content.

#### Favicon:

Can give indications of what framework is being used if the website is poorly maintained.

#### Sitemap.xml:

A file in the web application that lists every file the websites owner wants to show on a search engine.

#### HTTP Headers:

May contain the webservers software or scripting language in use. Accessible by running `curl -v ADDRESS`&#x20;

#### Extensions and Websites:

* Google Dorking: manipulating search parameters to find hidden pages.
* Wappalyzer (Extension): shows technologies of website.
* Wayback Machine: find old websites that may still be active.
* S3 Buckets (AWS storage service): can reveal information if permissions aren't properly set.
